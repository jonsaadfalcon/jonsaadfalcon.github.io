---
layout: paper
categories: papers
permalink: papers/weaver
id: weaver
title: "Shrinking the Generation-Verification Gap with Weak Verifiers"
authors: 
  - Jon Saad-Falcon
  - E. Kelly Buchanan
  - Mayee F. Chen
  - Tzu-Heng Huang
  - Brendan McLaughlin
  - Tanvir Bhathal
  - Shang Zhu
  - Ben Athiwaratkun
  - Frederic Sala
  - Scott Linderman
  - Azalia Mirhoseini
  - Christopher Ré
venue: NeurIPS
year: 2025
url: /papers/weaver
pdf: https://arxiv.org/abs/2506.18203
code: https://github.com/HazyResearch/scaling-verification
type: conference
figure: /images/papers/weaver_figure.png
image: /images/papers/weaver_image.png
featured: true
feature-order: 2
feature-title: "Weaver"
feature-description: "Combining Weak Verifiers for Strong Verification"
selected: true
bibtex: |-
  @misc{saadfalcon2025shrinkinggenerationverificationgapweak,
      title={Shrinking the Generation-Verification Gap with Weak Verifiers}, 
      author={Jon Saad-Falcon and E. Kelly Buchanan and Mayee F. Chen and Tzu-Heng Huang and Brendan McLaughlin and Tanvir Bhathal and Shang Zhu and Ben Athiwaratkun and Frederic Sala and Scott Linderman and Azalia Mirhoseini and Christopher Ré},
      year={2025},
      eprint={2506.18203},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
  }
  
---
Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and reward models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training. To reduce computational costs of verifier ensembles, we train a 400M cross-encoder using Weaver's combined output scores that retains 98.7% of Weaver's full accuracy while reducing verification compute by up to 99.97%.
